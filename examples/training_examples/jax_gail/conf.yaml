defaults:
  - override hydra/job_logging: default
  - override hydra/launcher: basic

#hydra:
#  mode: MULTIRUN
#  job_logging: {}
#  hydra_logging: {}
#  sweeper:
#    params:
#      experiment.learnable_std: true, false
#      experiment.init_std: 0.1, 0.2
#      experiment.num_steps: 10, 20
#      experiment.update_epochs: 4
#      experiment.clip_eps: 0.2, 0.05
#      # env_params
#      experiment.env_params.env_name: MjxUnitreeH1.walk, MjxUnitreeH1.run

wandb:
  project: "gail"

experiment:
  task_factory:
    name: ImitationFactory
    params:
      default_dataset_conf:
          task: walk
  env_params:
    env_name: BerkeleyHumanoidLite
    headless: true
    disable_arms: false
    horizon: 1000
    goal_type: GoalTrajMimic
    # the parameter proportion_env_reward decides how much the environment reward is used
    reward_type: MimicReward
    reward_params:
      qpos_w_sum: 0.0
      qvel_w_sum: 0.0
      rpos_w_sum: 0.5
      rquat_w_sum: 0.3
      rvel_w_sum: 0.1
      sites_for_mimic:
        - upper_body_mimic
        - left_hand_mimic
        - left_foot_mimic
        - right_hand_mimic
        - right_foot_mimic
  hidden_layers: [512, 256]
  lr: 1e-4
  disc_lr: 5e-5
  num_envs: 2048
  num_steps: 14
  total_timesteps: 75e6
  update_epochs: 4
  train_disc_interval: 3
  disc_minibatch_size: 2048
  proportion_env_reward: 0.0 # 0.0 means the policy receives no environment reward, just used for evaluation!
  n_disc_epochs: 10
  num_minibatches: 32
  gamma: 0.99
  gae_lambda: 0.95
  clip_eps: 0.2
  init_std: 0.125
  learnable_std: false
  ent_coef: 0.0
  disc_ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  activation: tanh
  anneal_lr: false
  weight_decay: 0.0
  normalize_env: true
  debug: false
  n_seeds: 1  # while automatically take seeds from 1 to n_seeds
  vmap_across_seeds: true
  validation:
    active: true
    num_steps: 100  # number of steps to run the validation --> this will create trajectories of length 100
    num_envs: 100   # number of environments to use for validation --> this will create 100 trajectories
    num: 10 # total number of evaluations done during training
    # if none of the following is set, only the mean reward and episode length is logged
    quantities: # quantities to be used for the distance measures below
      - JointPosition
      - JointVelocity
      - RelSitePosition
      - RelSiteVelocity
      - RelSiteOrientation
    measures:
      - EuclideanDistance
      - DynamicTimeWarping
      - DiscreteFrechetDistance
    rel_site_names:
      - upper_body_mimic
      - left_hand_mimic
      - left_foot_mimic
      - right_hand_mimic
      - right_foot_mimic
    joints_to_ignore:
      - pelvis_tx
      - pelvis_tz
      - pelvis_ty
      - pelvis_tilt
      - pelvis_list
      - pelvis_rotation